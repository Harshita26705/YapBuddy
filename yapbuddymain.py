# -*- coding: utf-8 -*-
"""YapBuddyMain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1051_5JRWcIAljwhc5RlJBhpLfUTyTWyD

1.Install Dependencies
"""

!pip install accelerate annotated-types anyio APScheduler cachetools certifi charset-normalizer colorama \
distro exceptiongroup filelock fsspec h11 httpcore httpx huggingface-hub idna Jinja2 jiter MarkupSafe mpmath \
nest-asyncio networkx numpy openai packaging psutil pydantic pydantic_core python-dotenv python-telegram-bot \
pytz PyYAML regex requests safetensors six sniffio sympy tokenizers torch tornado tqdm transformers \
typing-inspection typing_extensions tzdata tzlocal urllib3

"""2.Set Up Environment Variables"""

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN")

"""3.Import Required Libraries"""

# ðŸ“Œ Step 3: Import Required Libraries
import logging
import asyncio
import os
import torch
from transformers import pipeline  # ðŸ”¹ Ensures Hugging Face model initialization works
from telegram import Update
from telegram.ext import (
    ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler, filters
)

"""4.Configure Logging"""

logging.basicConfig(format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

"""5. Load API Tokens"""

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN")

"""6.Set Up GPU Optimization"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device set to: {device}")

""" 7. Initialize Hugging Face Model with Optimizations"""

from huggingface_hub import login
login(HUGGINGFACE_API_TOKEN)

pipe = pipeline(
    "text-generation",
    model="tiiuae/falcon-7b-instruct",
    device_map="auto",
    torch_dtype=torch.float16,
    model_kwargs={
        "temperature": 0.7,
        "top_p": 0.9,
        "max_length": 100,
        "do_sample": True  # âœ… Fixes warnings related to sampling
    }
)

"""8. Define Chatbot Handlers"""

import logging
import asyncio
from telegram import Update
from telegram.ext import ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler, filters

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("Hi! I'm YapBuddy ðŸ§ . I'm here to support you.")

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("Tell me what's on your mind, and I'll provide guidance.")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_message = update.message.text
    logger.info(f"Received message: {user_message}")

    # ðŸ”¹ Improved prompt for better personalization
    adjusted_prompt = f"You are YapBuddy, an empathetic AI therapist chatbot. A user says: '{user_message}'. Respond in a warm, supportive, and conversational tone."

    try:
        response = pipe(adjusted_prompt, max_new_tokens=75, return_full_text=False)[0]['generated_text']

        # ðŸ”¹ Remove prompt repetition and clean response
        filtered_response = response.replace(adjusted_prompt, "").strip()

        # ðŸ”¹ Improve sentence structure and ensure response sounds natural
        if filtered_response.endswith(("?", ".", "!")):
            final_response = filtered_response  # Keep as is if properly formatted
        else:
            final_response = filtered_response.split(".")[0] + "."  # Ensure complete sentence

        # ðŸ”¹ Dynamically adjust responses based on keywords for personalization
        if "exam" in user_message.lower():
            final_response += " You've prepared for thisâ€”trust yourself! ðŸ“šâœ¨"
        elif "fight" in user_message.lower():
            final_response += " Arguments can be tough, but open communication helps rebuild friendships. ðŸ’™"
        elif "sad" in user_message.lower():
            final_response += " You're not alone. I'm here for you, and things will get better. ðŸ’¡"
        else:
            final_response = filtered_response  # Default response

        await update.message.reply_text(final_response)
        logger.info(f"Sent response: {final_response}")

    except Exception as e:
        logger.error(f"Error generating response: {e}")
        await update.message.reply_text("I'm here for you. If something's on your mind, I'm happy to listen. ðŸ’™")

"""9. Run **YapBuddy**"""

async def main():
    application = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    await application.bot.delete_webhook(drop_pending_updates=True)
    logger.info("YapBuddy is running...")

    await application.run_polling()

"""10. Execute the Bot"""

import nest_asyncio
nest_asyncio.apply()

loop = asyncio.get_event_loop()
if loop.is_running():
    print("Event loop already running, scheduling bot start...")
    task = loop.create_task(main())  # Use create_task instead of run_until_complete
else:
    loop.run_until_complete(main())